'''This class contains fields and methods to extract weather data from desiredgeographic locations.'''import osimport requestsfrom bs4 import BeautifulSoupimport pandas as pdimport pickleimport jsonfrom datetime import datetimeclass WeatherData():        def __init__(self, noaa_api_key, city_code):                '''        Description:            Initialize CAMP User with relevant parameters                Inputs:            noaa_api_key: str                NOAA API key            city_code: str                city code for snow data                Outputs:            N/A        '''                self.noaa_api_key = noaa_api_key        self.city_code = city_code            def get_snow_table(self):                '''        Description:            Get snow totals by year from html                Inputs:            N/A                Outputs:            snow_table: pandas dataframe                table containing snow data        '''                # Get table from page        snow_url = os.path.join('https://www.weather.gov', self.city_code,                         'historicalSnow')        page = requests.get(snow_url)        soup = BeautifulSoup(page.text, 'lxml')        table = soup.find('table')                # Get headers        headers = []        for i in table.find_all('th'):            title = i.text            title = title.replace('\n', '')            if 'Min' in title or 'Max' in title or 'Mean' in title:                continue            headers.append(title)                # Create dataframe        snow_table = pd.DataFrame(columns = headers)                # Populate dataframe        for j in table.find_all('tr')[1:]:            row_data = j.find_all('td')            row = [i.text for i in row_data]            if row[0] == 'T':                break            length = len(snow_table)            snow_table.loc[length] = row                    return snow_table            def clean_snow_table(self, snow_table):                '''        Description:            Clean up snow table                Inputs:            snow_table: pandas dataframe                Outputs:            snow_table: pandas dataframe                cleaned table containing snow data        '''                # Get rid of any rows that are missing data (this year)        delete_rows = []        for r in range(len(snow_table)):            for c in range(len(snow_table.columns)):                data = snow_table.iloc[r, c]                if data == '\xa0':                    delete_rows.append(r)                    break                        snow_table = snow_table.drop(labels=delete_rows, axis=0)                # Get rid of all columns that aren'y year or snowfall total        keep_cols = ['Year', 'Season']        delete_cols = []        for c in snow_table.columns:            if c not in keep_cols:                delete_cols.append(c)                        snow_table = snow_table.drop(labels=delete_cols, axis=1)                # Convert to int/float        for r in range(len(snow_table)):            year = snow_table.iloc[r, 0].split('-')[0]            snow_table['Year'][r] = year                    snow_table['Year'] = snow_table['Year'].astype('int')        snow_table['Season'] = snow_table['Season'].astype('float')                return snow_table        def get_city_data_noaa(self, station_id, station_name, start_year, end_year):                '''        Description:            Get city data from NOAA API                Inputs:            station_id: str                station whose weather to access            station_name: str                location of station            start_year: int                year to start grabbing data            end_year: int                year to end grabbing data                Outputs:            N/A: saves json files to data repo        '''                # List of fields we want from GHCND dataset        # https://www.ncei.noaa.gov/pub/data/ghcn/daily/readme.txt        fields = ['PRCP', # Precipitation (tenths of mm)                  'SNOW', # Snowfall (mm)                  'SNWD', # Snow depth (mm)                  'TMAX', # Maximum temperature (tenths of degrees C)                  'TMIN', # Minimum temperature (tenths of degrees C)                  'ACMC', # Average cloudiness midnight to midnight from 30-second ceilometer data (percent)                  'ACMH', # Average cloudiness midnight to midnight from manual observations (percent)                  'ADPT', # Average Dew Point Temperature for the day (tenths of degrees C)                  'ASLP', # Average Sea Level Pressure for the day (hPa * 10)                  'ASTP', # Average Station Level Pressure for the day (hPa * 10)                  'AWDR', # Average daily wind direction (degrees)                  'AWND', # Average daily wind speed (tenths of meters per second)                  'EVAP', # Evaporation of water from evaporation pan (tenths of mm)                  'FRGB', # Base of frozen ground layer (cm)                  'FRGT', # Top of frozen ground layer (cm)                  'PSUN', # Daily percent of possible sunshine (percent)                  'RHAV', # Average relative humidity for the day (percent)                  'RHMN', # Minimum relative humidity for the day (percent)                  'RHMX', # Maximum relative humidity for the day (percent)                  'TAVG', # Average temperature (tenths of degrees C)                  'TSUN', # Daily total sunshine (minutes)                  'WSFG'] # WSFG = Peak gust wind speed (tenths of meters per second)                for year in range(start_year, end_year + 1):                        year = str(year)                        # Need to be careful because NOAA has 1000 limit for query            for f in fields:                url = 'https://www.ncdc.noaa.gov/cdo-web/api/' + \                    'v2/data?datasetid=GHCND&datatypeid=' + f + \                    '&limit=1000&stationid=' + station_id + \                    '&startdate=' + year + '-01-01&enddate=' + year + '-12-31'                r = requests.get(url, headers={'token':self.noaa_api_key})                json_data = r.json()                                # Skip if empty                if len(json_data.keys()) > 0:                                    # Save dataframe otherwise                    save_loc = os.path.join(os.getcwd(), 'api_data')                    if not os.path.isdir(save_loc):                        os.mkdir(save_loc)                    fname = station_name + '_' + f + '_' + year + '.json'                    with open(os.path.join(save_loc, fname), 'w') as f:                        json.dump(json_data, f)                            def get_city_data_open_meteo(self, lat, lon, name, start_year, end_year,                                 save_loc):                '''        Description:            Get city data from Open-Meteo API                Inputs:            lat: float                city latitude            lon: float                city longitude            name: str                city name            start_year: int                year to start grabbing data            end_year: int                year to end grabbing data            save_loc: str                where to save json files                Outputs:            N/A: saves json files to data repo        '''                if not os.path.isdir(save_loc):            os.mkdir(save_loc)                # List of fields we want from historical dataset        # https://open-meteo.com/en/docs/historical-weather-api#api_form                fields = ['temperature_2m',                  'relativehumidity_2m',                  'apparent_temperature',                  'surface_pressure',                  'snowfall',                  'shortwave_radiation',                  'direct_radiation',                  'diffuse_radiation',                  'direct_normal_irradiance',                  'windspeed_10m',                  'winddirection_10m',                  'et0_fao_evapotranspiration',                  'soil_temperature_0_to_7cm',                  'soil_moisture_0_to_7cm']                # Build fields string for API query        fields_str = ''        for f in fields:            fields_str = fields_str + f + ','        fields_str = fields_str[0:-1]        for year in range(start_year, end_year + 1):                        # Get info for API query            year = str(year)            sd = year + '-01-01'            ed = year + '-12-31'            url = 'https://archive-api.open-meteo.com/v1/archive?latitude=' + \                str(lat) + '&longitude=' + str(lon) + \                '&start_date=' + sd + '&end_date=' + ed + \                '&hourly=' + fields_str + '&timezone=America%2FNew_York'                            # Make query            r = requests.get(url)                        # Check for error code            if r.status_code != 200:                print('Error code in API query.')                continue                        # Convert to json            json_data = r.json()                        # Skip if empty            if len(json_data.keys()) > 0:                            # Save dataframe otherwise                fname = name + '_' + year + '.json'                with open(os.path.join(save_loc, fname), 'w') as f:                    json.dump(json_data, f)                        def clean_city_data_open_meteo(self, name, json_loc, save_loc):                '''        Description:            Clean city data from Open-Meteo API                Inputs:            name: str                city name            json_loc : str                file path to json files            save_loc : str                file path to save dataframe                Outputs:            N/A: saves pandas dataframe to repo with pickle        '''                if not os.path.isdir(save_loc):            os.mkdir(save_loc)                # Get path for all json files with 'name' in it        all_json_files = os.listdir(json_loc)        matched_json_files = [x for x in all_json_files if name in x]                # Loop through and convert to pandas dataframe, then append        for idx, j in enumerate(matched_json_files):                        full_file = os.path.join(json_loc, j)            f = open(full_file)            data = json.load(f)                        if idx == 0:                df_full = pd.DataFrame(data['hourly'])            else:                df_tmp = pd.DataFrame(data['hourly'])                df_full = df_full.append(df_tmp, ignore_index=True)                # Add a new column 'days_since_Jan1_1900'        time = list(df_full['time'])        days_since_Jan1_1900 = []        t_1900 = datetime(2000, 1, 1, 0, 0)        sec_per_day = 24 * 60 * 60        for t in time:            t_entry = datetime.strptime(t.replace('T', ' '), '%Y-%m-%d %H:%M')            t_diff = t_entry - t_1900            days = t_diff.days            secs = t_diff.seconds            days_since_Jan1_1900.append(days + secs / sec_per_day)                    # Add column to dataframe, sort, and reset indices        df_full['days_since_Jan1_2000'] = days_since_Jan1_1900        df_full = df_full.sort_values(by=['days_since_Jan1_2000'])        df_full = df_full.reset_index(drop=True)                # Save with pickle        file_name = name + '.pkl'        full_file = os.path.join(save_loc, file_name)        with open(full_file, 'wb') as handle:            pickle.dump(df_full, handle, protocol=pickle.HIGHEST_PROTOCOL)            if __name__ == "__main__":        from matplotlib import pyplot as plt        # Get some folder info    pwd = os.getcwd()    save_loc = os.path.join(pwd, 'figs')    if not os.path.isdir(save_loc):        os.mkdir(save_loc)        # Initialize WeatherData object    obj = WeatherData('WFOckfmIedXAYNpnsZkatzGzyFkxUvWd', 'btv')        # Get Burlington snow data and plot    if not os.path.isfile(os.path.join(pwd, 'figs', 'btv_snowfall_clean.png')):                # Get Burlington snow data        snow_table = obj.get_snow_table()        clean_snow_table = obj.clean_snow_table(snow_table)                # Plot uncleaned Burlington snow data        fig, ax = plt.subplots()        x = list(snow_table['Year'])        y = list(snow_table['Season'].astype('float'))        ax.plot(x, y)        my_xticks = ax.get_xticks()        ax.set_xticks([my_xticks[0], my_xticks[round(my_xticks[-1] / 2)],                       my_xticks[-1]], visible=True, rotation=45)        ax.set_xlabel('Year')        ax.set_ylabel('Snowfall [in]')        ax.set_title('Burlington, VT Yearly Snowfall: Uncleaned')        ax.grid()        fig.savefig(os.path.join(save_loc, 'btv_snowfall_unclean.png'), dpi=600)                # Plot cleaned Burlington snow data        fig, ax = plt.subplots()        x = list(clean_snow_table['Year'])        y = list(clean_snow_table['Season'].astype('float'))        ax.plot(x, y)        ax.set_xlabel('Year')        ax.set_ylabel('Snowfall [in]')        ax.set_title('Burlington, VT Yearly Snowfall: Cleaned')        ax.grid()        fig.savefig(os.path.join(save_loc, 'btv_snowfall_clean.png'), dpi=600)        # Get data from different Vermont cities - Open-Meteo    latlon = [[44.48, -73.21], # Burlington              [44.26, -72.58], # Montpelier              [43.61, -72.97], # Rutland              [42.85, -72.56], # Brattleboro              [44.95, -72.31], # Newport              [44.47, -72.68]] # Stowe    station_names = ['Burlington',                     'Montpelier',                     'Rutland',                     'Brattleboro',                     'Newport',                     'Stowe']        # Only get data if it doesn't exist    save_loc = os.path.join(pwd, 'api_data')    if not os.path.isfile(os.path.join(save_loc, 'Burlington_2020.json')):        for (ll, sn) in zip(latlon, station_names):            obj.get_city_data_open_meteo(ll[0], ll[1], sn, 2013, 2023, save_loc)                # Clean data if pickle files don't exist    json_loc = os.path.join(pwd, 'api_data')    save_loc = os.path.join(pwd, 'api_data_clean')    if not os.path.isfile(os.path.join(save_loc, 'Burlington.pkl')):        for name in station_names:            obj.clean_city_data_open_meteo(name, json_loc, save_loc)                # Now make some plots    save_loc = os.path.join(pwd, 'figs')    if not os.path.isdir(save_loc):        os.mkdir(save_loc)        # Only plot if we haven't already    if not os.path.isfile(os.path.join(save_loc, 'temperature_2m.png')):                data = [None] * len(station_names)        for i, name in enumerate(station_names):            with open(os.path.join(pwd, 'api_data_clean', name + '.pkl'), 'rb') as handle:                data[i] = pickle.load(handle)                    keys = data[0].keys()        keys = list(keys[1:-1])                labels = ['Temperature [C]',                  'Relative Humidity [%]',                  'Apparent Temperature [C]',                  'Surface Pressure [hPa]',                  'Snowfall [cm]',                  'Shortwave Radiation [W/m^2]',                  'Direct Radiation [W/m^2]',                  'Diffuse Radiation [W/m^2]',                  'Direct Normal Irradiance [W/m^2]',                  'Wind Speed [km/hr]',                  'Wind Direction [deg]',                  'Reference Evapotranspiration [mm]',                  'Soil Temperature: 0 to 7cm [C]',                  'Soil Moisture: 0 to 7cm [m^3/m^3]']                plt.rcParams.update({'font.size': 28})        colors = ['b', 'g', 'r', 'c', 'm', 'k']        for idx, k in enumerate(keys):            fig, ax = plt.subplots(6, 1, figsize=(12, 20))            for i, name in enumerate(station_names):                ax[i].plot(data[i]['days_since_Jan1_2000'], data[i][k],                    linewidth=1, color=colors[i], label=name, alpha=1)                ax[i].grid()                ax[i].legend(loc='upper left')            fig.add_subplot(111, frameon=False)            plt.tick_params(labelcolor='none', which='both',                 top=False, bottom=False, left=False, right=False)            plt.xlabel('Days Since 1/1/2000')            plt.ylabel(labels[idx])            plt.title('Data Over Time')            fig.savefig(os.path.join(save_loc, keys[idx] + '.png'), dpi=600)                                                                